# .github/workflows/main.yml
# Comprehensive CI/CD Pipeline with DevOps Best Practices
# Auto-generated by P.O.C.E. Project Creator v4.0

name: ðŸš€ Complete DevOps Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 2 * * 1'  # Weekly security scan
  workflow_dispatch:
    inputs:
      environment:
        description: 'Target environment'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
      skip_tests:
        description: 'Skip test execution'
        required: false
        default: false
        type: boolean

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.11'
  CACHE_VERSION: 'v1'
  
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # ==========================================
  # PHASE 1: VALIDATION AND SETUP
  # ==========================================
  
  setup:
    name: ðŸ”§ Setup & Validation
    runs-on: ubuntu-latest
    timeout-minutes: 10
    outputs:
      cache-key: ${{ steps.cache-keys.outputs.cache-key }}
      should-deploy: ${{ steps.conditions.outputs.should-deploy }}
      test-matrix: ${{ steps.test-matrix.outputs.matrix }}
    
    steps:
      - name: ðŸ“¥ Checkout Repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for better analysis
      
      - name: ðŸ”‘ Cache Keys Generation
        id: cache-keys
        run: |
          echo "cache-key=deps-${{ env.CACHE_VERSION }}-${{ runner.os }}-${{ hashFiles('**/package*.json', '**/requirements.txt', '**/pyproject.toml') }}" >> $GITHUB_OUTPUT
      
      - name: ðŸŽ¯ Deployment Conditions
        id: conditions
        run: |
          if [[ "${{ github.ref }}" == "refs/heads/main" ]] && [[ "${{ github.event_name }}" == "push" ]]; then
            echo "should-deploy=true" >> $GITHUB_OUTPUT
          else
            echo "should-deploy=false" >> $GITHUB_OUTPUT
          fi
      
      - name: ðŸ§ª Test Matrix Setup
        id: test-matrix
        run: |
          echo 'matrix={"os": ["ubuntu-latest", "windows-latest", "macos-latest"], "version": ["3.9", "3.10", "3.11"]}' >> $GITHUB_OUTPUT

  # ==========================================
  # PHASE 2: CODE QUALITY AND SECURITY
  # ==========================================
  
  code-quality:
    name: ðŸ” Code Quality Analysis
    runs-on: ubuntu-latest
    needs: setup
    timeout-minutes: 15
    
    steps:
      - name: ðŸ“¥ Checkout
        uses: actions/checkout@v4
      
      - name: ðŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: ðŸ“¦ Install Dependencies
        run: |
          pip install --upgrade pip
          pip install flake8 black isort mypy bandit safety pylint
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          if [ -f requirements-dev.txt ]; then pip install -r requirements-dev.txt; fi
      
      - name: ðŸŽ¨ Code Formatting Check
        run: |
          black --check --diff .
          isort --check-only --diff .
      
      - name: ðŸ“ Linting
        run: |
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
      
      - name: ðŸ”’ Type Checking
        run: mypy . --ignore-missing-imports || true
      
      - name: ðŸ“Š Code Complexity Analysis
        run: |
          pylint **/*.py --exit-zero --output-format=text --reports=yes > pylint-report.txt
          cat pylint-report.txt
      
      - name: ðŸ“ˆ Upload Code Quality Reports
        uses: actions/upload-artifact@v3
        with:
          name: code-quality-reports
          path: |
            pylint-report.txt
          retention-days: 30

  security-scan:
    name: ðŸ›¡ï¸ Security Analysis
    runs-on: ubuntu-latest
    needs: setup
    timeout-minutes: 20
    permissions:
      security-events: write
      actions: read
      contents: read
    
    steps:
      - name: ðŸ“¥ Checkout
        uses: actions/checkout@v4
      
      - name: ðŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: ðŸ”’ Install Security Tools
        run: |
          pip install bandit safety semgrep
      
      - name: ðŸš¨ Bandit Security Scan
        run: |
          bandit -r . -f json -o bandit-report.json || true
          bandit -r . -f txt
      
      - name: ðŸ” Safety Dependency Check
        run: |
          safety check --json --output safety-report.json || true
          safety check
      
      - name: âš¡ Semgrep Static Analysis
        run: |
          semgrep --config=auto --json --output=semgrep-report.json . || true
          semgrep --config=auto .
      
      - name: ðŸ“Š CodeQL Analysis
        uses: github/codeql-action/init@v2
        with:
          languages: python
      
      - name: ðŸ—ï¸ CodeQL Autobuild
        uses: github/codeql-action/autobuild@v2
      
      - name: ðŸ“ˆ CodeQL Analysis
        uses: github/codeql-action/analyze@v2
      
      - name: ðŸ“„ Upload Security Reports
        uses: actions/upload-artifact@v3
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json
            semgrep-report.json
          retention-days: 90

  # ==========================================
  # PHASE 3: COMPREHENSIVE TESTING
  # ==========================================
  
  unit-tests:
    name: ðŸ§ª Unit Tests
    runs-on: ${{ matrix.os }}
    needs: [setup, code-quality]
    if: ${{ !inputs.skip_tests }}
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.9', '3.10', '3.11']
        exclude:
          - os: windows-latest
            python-version: '3.9'
          - os: macos-latest
            python-version: '3.9'
    
    steps:
      - name: ðŸ“¥ Checkout
        uses: actions/checkout@v4
      
      - name: ðŸ Setup Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
      
      - name: ðŸ“¦ Install Dependencies
        run: |
          pip install --upgrade pip
          pip install pytest pytest-cov pytest-xdist pytest-mock
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
          if [ -f requirements-test.txt ]; then pip install -r requirements-test.txt; fi
        shell: bash
      
      - name: ðŸ§ª Run Unit Tests
        run: |
          pytest tests/unit/ -v --cov=src --cov-report=xml --cov-report=html --junitxml=pytest-results.xml
        shell: bash
      
      - name: ðŸ“Š Upload Coverage to Codecov
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false
      
      - name: ðŸ“ˆ Upload Test Results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: test-results-${{ matrix.os }}-${{ matrix.python-version }}
          path: |
            pytest-results.xml
            htmlcov/
          retention-days: 30

  integration-tests:
    name: ðŸ”— Integration Tests
    runs-on: ubuntu-latest
    needs: [setup, unit-tests]
    if: ${{ !inputs.skip_tests }}
    timeout-minutes: 45
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: testdb
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
      
      redis:
        image: redis:7
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
    
    steps:
      - name: ðŸ“¥ Checkout
        uses: actions/checkout@v4
      
      - name: ðŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'
      
      - name: ðŸ“¦ Install Dependencies
        run: |
          pip install --upgrade pip
          pip install pytest pytest-asyncio pytest-docker
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi
      
      - name: ðŸ”§ Setup Test Environment
        run: |
          echo "DATABASE_URL=postgresql://postgres:postgres@localhost:5432/testdb" >> $GITHUB_ENV
          echo "REDIS_URL=redis://localhost:6379" >> $GITHUB_ENV
      
      - name: ðŸ§ª Run Integration Tests
        run: |
          pytest tests/integration/ -v --junitxml=integration-results.xml
      
      - name: ðŸ“ˆ Upload Integration Results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: integration-test-results
          path: integration-results.xml
          retention-days: 30

  e2e-tests:
    name: ðŸŽ­ End-to-End Tests
    runs-on: ubuntu-latest
    needs: [setup, integration-tests]
    if: ${{ !inputs.skip_tests && github.event_name != 'schedule' }}
    timeout-minutes: 60
    
    steps:
      - name: ðŸ“¥ Checkout
        uses: actions/checkout@v4
      
      - name: ðŸŽ­ Setup Playwright
        uses: microsoft/playwright-github-action@v1
      
      - name: ðŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: ðŸ“¦ Install Dependencies
        run: |
          pip install playwright pytest-playwright
          playwright install chromium firefox webkit
      
      - name: ðŸš€ Start Application
        run: |
          # Start your application in background
          python app.py &
          sleep 10  # Wait for app to start
      
      - name: ðŸŽ­ Run E2E Tests
        run: |
          pytest tests/e2e/ -v --browser chromium --browser firefox
      
      - name: ðŸ“¸ Upload Screenshots
        uses: actions/upload-artifact@v3
        if: failure()
        with:
          name: e2e-screenshots
          path: test-results/
          retention-days: 30

  # ==========================================
  # PHASE 4: PERFORMANCE AND LOAD TESTING
  # ==========================================
  
  performance-tests:
    name: âš¡ Performance Testing
    runs-on: ubuntu-latest
    needs: [setup, integration-tests]
    if: ${{ github.ref == 'refs/heads/main' }}
    timeout-minutes: 30
    
    steps:
      - name: ðŸ“¥ Checkout
        uses: actions/checkout@v4
      
      - name: ðŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: ðŸ“¦ Install Load Testing Tools
        run: |
          pip install locust pytest-benchmark
      
      - name: ðŸš€ Start Application
        run: |
          python app.py &
          sleep 15
      
      - name: âš¡ Run Performance Tests
        run: |
          # Benchmark tests
          pytest tests/performance/ --benchmark-json=benchmark-results.json
          
          # Load testing with Locust
          locust -f tests/load/locustfile.py --headless -u 100 -r 10 -t 300s --html=load-test-report.html
      
      - name: ðŸ“Š Upload Performance Reports
        uses: actions/upload-artifact@v3
        with:
          name: performance-reports
          path: |
            benchmark-results.json
            load-test-report.html
          retention-days: 90

  # ==========================================
  # PHASE 5: BUILD AND PACKAGE
  # ==========================================
  
  build:
    name: ðŸ—ï¸ Build & Package
    runs-on: ubuntu-latest
    needs: [code-quality, security-scan, unit-tests]
    timeout-minutes: 20
    outputs:
      image-tag: ${{ steps.image.outputs.tag }}
      package-version: ${{ steps.version.outputs.version }}
    
    steps:
      - name: ðŸ“¥ Checkout
        uses: actions/checkout@v4
      
      - name: ðŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: ðŸ“¦ Install Build Tools
        run: |
          pip install build twine wheel
      
      - name: ðŸ”¢ Generate Version
        id: version
        run: |
          VERSION=$(python setup.py --version 2>/dev/null || echo "1.0.0")
          echo "version=$VERSION" >> $GITHUB_OUTPUT
          echo "Generated version: $VERSION"
      
      - name: ðŸ—ï¸ Build Package
        run: |
          python -m build
      
      - name: ðŸ§ª Test Package Installation
        run: |
          pip install dist/*.whl
          python -c "import your_package; print('Package installed successfully')"
      
      - name: ðŸ³ Set up Docker Buildx
        uses: docker/setup-buildx-action@v2
      
      - name: ðŸ”‘ Login to Container Registry
        uses: docker/login-action@v2
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}
      
      - name: ðŸ·ï¸ Generate Image Tags
        id: image
        run: |
          IMAGE_TAG=ghcr.io/${{ github.repository }}:${{ steps.version.outputs.version }}
          echo "tag=$IMAGE_TAG" >> $GITHUB_OUTPUT
      
      - name: ðŸ³ Build and Push Docker Image
        uses: docker/build-push-action@v4
        with:
          context: .
          push: true
          tags: |
            ${{ steps.image.outputs.tag }}
            ghcr.io/${{ github.repository }}:latest
          cache-from: type=gha
          cache-to: type=gha,mode=max
      
      - name: ðŸ“¦ Upload Build Artifacts
        uses: actions/upload-artifact@v3
        with:
          name: build-artifacts
          path: |
            dist/
            Dockerfile
          retention-days: 30

  # ==========================================
  # PHASE 6: DEPLOYMENT
  # ==========================================
  
  deploy-staging:
    name: ðŸš€ Deploy to Staging
    runs-on: ubuntu-latest
    needs: [build, integration-tests]
    if: ${{ needs.setup.outputs.should-deploy == 'true' || github.event.inputs.environment == 'staging' }}
    environment: 
      name: staging
      url: https://staging.example.com
    timeout-minutes: 15
    
    steps:
      - name: ðŸ“¥ Checkout
        uses: actions/checkout@v4
      
      - name: ðŸš€ Deploy to Staging
        run: |
          echo "Deploying ${{ needs.build.outputs.image-tag }} to staging..."
          # Add your deployment logic here
          # kubectl apply -f k8s/staging/
          # helm upgrade --install staging-app ./helm-chart
      
      - name: ðŸ§ª Smoke Tests
        run: |
          echo "Running smoke tests against staging..."
          curl -f https://staging.example.com/health || exit 1
      
      - name: ðŸ“Š Deployment Notification
        if: always()
        run: |
          echo "Staging deployment completed with status: ${{ job.status }}"

  deploy-production:
    name: ðŸŒŸ Deploy to Production
    runs-on: ubuntu-latest
    needs: [build, deploy-staging, performance-tests]
    if: ${{ github.ref == 'refs/heads/main' && github.event_name == 'push' || github.event.inputs.environment == 'production' }}
    environment: 
      name: production
      url: https://production.example.com
    timeout-minutes: 30
    
    steps:
      - name: ðŸ“¥ Checkout
        uses: actions/checkout@v4
      
      - name: ðŸ›¡ï¸ Security Check
        run: |
          echo "Final security verification..."
          # Add production security checks
      
      - name: ðŸš€ Blue-Green Deployment
        run: |
          echo "Deploying ${{ needs.build.outputs.image-tag }} to production..."
          # Implement blue-green deployment strategy
          # kubectl apply -f k8s/production/
      
      - name: ðŸ§ª Production Health Check
        run: |
          echo "Verifying production deployment..."
          for i in {1..10}; do
            if curl -f https://production.example.com/health; then
              echo "Health check passed"
              break
            fi
            echo "Attempt $i failed, retrying..."
            sleep 30
          done
      
      - name: ðŸ“Š Performance Verification
        run: |
          echo "Verifying production performance..."
          # Add performance verification logic
      
      - name: ðŸŽ‰ Deployment Success Notification
        if: success()
        run: |
          echo "ðŸŽ‰ Production deployment successful!"
          echo "Version: ${{ needs.build.outputs.package-version }}"
          echo "Image: ${{ needs.build.outputs.image-tag }}"

  # ==========================================
  # PHASE 7: POST-DEPLOYMENT MONITORING
  # ==========================================
  
  post-deployment:
    name: ðŸ“Š Post-Deployment Monitoring
    runs-on: ubuntu-latest
    needs: [deploy-production]
    if: ${{ always() && needs.deploy-production.result == 'success' }}
    timeout-minutes: 10
    
    steps:
      - name: ðŸ“Š Setup Monitoring
        run: |
          echo "Setting up post-deployment monitoring..."
          # Configure monitoring alerts
          # Update dashboards
          # Set up log aggregation
      
      - name: ðŸ“ˆ Performance Baseline
        run: |
          echo "Establishing performance baselines..."
          # Capture initial performance metrics
      
      - name: ðŸ”” Alert Configuration
        run: |
          echo "Configuring alerting rules..."
          # Set up monitoring alerts for the new deployment

  # ==========================================
  # REPORTING AND CLEANUP
  # ==========================================
  
  report:
    name: ðŸ“‹ Generate Reports
    runs-on: ubuntu-latest
    needs: [code-quality, security-scan, unit-tests, integration-tests, build]
    if: always()
    timeout-minutes: 10
    
    steps:
      - name: ðŸ“¥ Download All Artifacts
        uses: actions/download-artifact@v3
      
      - name: ðŸ“Š Generate Comprehensive Report
        run: |
          echo "# ðŸ“‹ Pipeline Execution Report" > pipeline-report.md
          echo "" >> pipeline-report.md
          echo "**Execution Date:** $(date)" >> pipeline-report.md
          echo "**Commit:** ${{ github.sha }}" >> pipeline-report.md
          echo "**Branch:** ${{ github.ref_name }}" >> pipeline-report.md
          echo "" >> pipeline-report.md
          
          echo "## ðŸ† Summary" >> pipeline-report.md
          echo "- âœ… Code Quality: ${{ needs.code-quality.result }}" >> pipeline-report.md
          echo "- ðŸ›¡ï¸ Security Scan: ${{ needs.security-scan.result }}" >> pipeline-report.md
          echo "- ðŸ§ª Unit Tests: ${{ needs.unit-tests.result }}" >> pipeline-report.md
          echo "- ðŸ”— Integration Tests: ${{ needs.integration-tests.result }}" >> pipeline-report.md
          echo "- ðŸ—ï¸ Build: ${{ needs.build.result }}" >> pipeline-report.md
          
          cat pipeline-report.md
      
      - name: ðŸ“ˆ Upload Final Report
        uses: actions/upload-artifact@v3
        with:
          name: pipeline-report
          path: pipeline-report.md
          retention-days: 90
      
      - name: ðŸ’¬ Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('pipeline-report.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });

  cleanup:
    name: ðŸ§¹ Cleanup
    runs-on: ubuntu-latest
    needs: [report]
    if: always()
    timeout-minutes: 5
    
    steps:
      - name: ðŸ§¹ Cleanup Temporary Resources
        run: |
          echo "Cleaning up temporary resources..."
          # Clean up any temporary resources
          # Remove old artifacts if needed
          # Clean up test environments
      
      - name: ðŸ“Š Update Metrics
        run: |
          echo "Updating pipeline metrics..."
          # Update pipeline success/failure metrics
          # Log execution times
          # Update quality scores